{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_classifier import CIFAR10Classifier\n",
    "import config\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import set_seed, set_deterministic\n",
    "set_seed(config.SEED)\n",
    "set_deterministic(\n",
    "    deterministic=config.DETERMINISTIC,\n",
    "    benchmark=config.BENCHMARK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import (\n",
    "    compute_mean_std, get_transforms,\n",
    "    load_cifar10_datasets, split_train_val, create_loaders,\n",
    "    get_dataset_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading/loading CIFAR-10 datasets...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "âœ… Loaded training samples: 50000, test samples: 10000\n",
      "ğŸ“Š Computing mean and std...\n",
      "âœ… Mean: [0.4913996756076813, 0.48215851187705994, 0.4465310275554657], Std: [0.24703219532966614, 0.24348489940166473, 0.2615877091884613]\n",
      "ğŸ§ª Creating normalization transform...\n",
      "ğŸ“¥ Downloading/loading CIFAR-10 datasets...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "âœ… Loaded training samples: 50000, test samples: 10000\n",
      "ğŸ”€ Splitting dataset with ratio 0.80...\n",
      "âœ… Train size: 40000, Validation size: 10000\n",
      "ğŸ“¦ Creating data loaders with batch size 256...\n",
      "âœ… Data loaders ready.\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "raw_dataset, _ = load_cifar10_datasets(transform=transforms.ToTensor())\n",
    "mean, std = compute_mean_std(raw_dataset)\n",
    "\n",
    "# Apply transformations\n",
    "full_transform = get_transforms(mean, std)\n",
    "\n",
    "# Load with transformations\n",
    "train_dataset, test_dataset = load_cifar10_datasets(transform=full_transform)\n",
    "\n",
    "# Split\n",
    "train_subset, val_subset = split_train_val(train_dataset, split_ratio=config.SPLIT_RATIO)\n",
    "\n",
    "# Loaders\n",
    "train_loader, val_loader, test_loader = create_loaders(train_subset, val_subset, test_dataset, batch_size=config.BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Extracting dataset info...\n",
      "âœ… Input shape: torch.Size([3, 32, 32]), Number of classes: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CIFAR10_torch                            [1, 10]                   --\n",
       "â”œâ”€Flatten: 1-1                           [1, 3072]                 --\n",
       "â”œâ”€Sequential: 1-2                        [1, 64]                   --\n",
       "â”‚    â””â”€Linear: 2-1                       [1, 1024]                 3,146,752\n",
       "â”‚    â””â”€BatchNorm1d: 2-2                  [1, 1024]                 2,048\n",
       "â”‚    â””â”€ReLU: 2-3                         [1, 1024]                 --\n",
       "â”‚    â””â”€Dropout: 2-4                      [1, 1024]                 --\n",
       "â”‚    â””â”€Linear: 2-5                       [1, 512]                  524,800\n",
       "â”‚    â””â”€BatchNorm1d: 2-6                  [1, 512]                  1,024\n",
       "â”‚    â””â”€ReLU: 2-7                         [1, 512]                  --\n",
       "â”‚    â””â”€Dropout: 2-8                      [1, 512]                  --\n",
       "â”‚    â””â”€Linear: 2-9                       [1, 256]                  131,328\n",
       "â”‚    â””â”€BatchNorm1d: 2-10                 [1, 256]                  512\n",
       "â”‚    â””â”€ReLU: 2-11                        [1, 256]                  --\n",
       "â”‚    â””â”€Dropout: 2-12                     [1, 256]                  --\n",
       "â”‚    â””â”€Linear: 2-13                      [1, 128]                  32,896\n",
       "â”‚    â””â”€BatchNorm1d: 2-14                 [1, 128]                  256\n",
       "â”‚    â””â”€ReLU: 2-15                        [1, 128]                  --\n",
       "â”‚    â””â”€Dropout: 2-16                     [1, 128]                  --\n",
       "â”‚    â””â”€Linear: 2-17                      [1, 64]                   8,256\n",
       "â”‚    â””â”€BatchNorm1d: 2-18                 [1, 64]                   128\n",
       "â”‚    â””â”€ReLU: 2-19                        [1, 64]                   --\n",
       "â”‚    â””â”€Dropout: 2-20                     [1, 64]                   --\n",
       "â”œâ”€Linear: 1-3                            [1, 10]                   650\n",
       "==========================================================================================\n",
       "Total params: 3,848,650\n",
       "Trainable params: 3,848,650\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 3.85\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 15.39\n",
       "Estimated Total Size (MB): 15.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_shape, num_classes = get_dataset_info(train_dataset)\n",
    "\n",
    "model_cls = CIFAR10Classifier(\n",
    "    name=\"torch_1024_5\",\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    hidden_layers=[1024,512,256,128,64],\n",
    "    dropout_rate=[0.3,0.3,0.3,0.3,0.3],\n",
    "    activation_fn_name=\"ReLU\",\n",
    "    device=device\n",
    "    )\n",
    "model_cls.build_model()\n",
    "model_cls.compile()\n",
    "model_cls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Training configuration:\n",
      "ğŸ“¦ Model name:        torch_1024_5\n",
      "ğŸ“ Input shape:       torch.Size([3, 32, 32])\n",
      "ğŸ”¢ Hidden layers:     [1024, 512, 256, 128, 64]\n",
      "ğŸ› Dropout rates:     [0.3, 0.3, 0.3, 0.3, 0.3]\n",
      "âš™ï¸ Activation:        ReLU\n",
      "ğŸ“ˆ Optimizer:         Adam {'lr': 0.01}\n",
      "ğŸ¯ Criterion:         CrossEntropyLoss {}\n",
      "ğŸ§  Device:            cuda\n",
      "ğŸ“Š Epochs:            10\n",
      "ğŸª„ Early stopping:    True (patience=10)\n",
      "============================================================\n",
      "\n",
      "[1/10] Train loss: 1.8749, acc: 0.3171 | Val loss: 1.6738, acc: 0.3940 | ğŸ•’ 21.31s\n",
      "[2/10] Train loss: 1.6860, acc: 0.3957 | Val loss: 1.5758, acc: 0.4336 | ğŸ•’ 20.88s\n",
      "[3/10] Train loss: 1.5857, acc: 0.4358 | Val loss: 1.5006, acc: 0.4659 | ğŸ•’ 20.79s\n",
      "[4/10] Train loss: 1.5204, acc: 0.4602 | Val loss: 1.4702, acc: 0.4835 | ğŸ•’ 20.90s\n",
      "[5/10] Train loss: 1.4647, acc: 0.4827 | Val loss: 1.4436, acc: 0.4830 | ğŸ•’ 21.16s\n",
      "[6/10] Train loss: 1.4199, acc: 0.4996 | Val loss: 1.4225, acc: 0.4927 | ğŸ•’ 20.76s\n",
      "[7/10] Train loss: 1.3840, acc: 0.5134 | Val loss: 1.3832, acc: 0.5122 | ğŸ•’ 20.69s\n",
      "[8/10] Train loss: 1.3454, acc: 0.5290 | Val loss: 1.3754, acc: 0.5218 | ğŸ•’ 21.08s\n",
      "[9/10] Train loss: 1.3107, acc: 0.5397 | Val loss: 1.3723, acc: 0.5178 | ğŸ•’ 22.76s\n",
      "[10/10] Train loss: 1.2837, acc: 0.5476 | Val loss: 1.3474, acc: 0.5276 | ğŸ•’ 21.93s\n",
      "\n",
      "âœ… Training finished!\n",
      "ğŸ•’ Total training time: 212.73 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model_cls.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    log_tensorboard=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv Homework)",
   "language": "python",
   "name": "venv-homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
